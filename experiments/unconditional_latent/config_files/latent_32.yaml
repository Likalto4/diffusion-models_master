# configuration file for the unconditional training

# 1. Image processing
processing:
  # dataset name
  dataset: "data/images/breast10p_latents" # name of the dataset or directory in repo
  # image size
  resolution: 64
  # interpolation: "BILINEAR"
  # # normalisation
  # normalisation_value: 4095.0
  # batch size for dataloader
  batch_size: 32
  # number of workers for dataloader
  num_workers: 12 # 0 means no extra processes are used (run in main process)

# 2. Model
model:
  in_channels: 4 # The number of channels to the Unet
  out_channels: 4 # The number of channels from the Unet
  layers_per_block: 2 # How many ResNet layers to use in each Unet block
  block_out_channels: !!python/tuple # The output channels for each block # More channels -> more parameters # The length of this tuple is the number of blocks
  - 128
  - 256
  - 256
  - 512  
  down_block_types: !!python/tuple # Describes the type of block to use for downsampling
  - "DownBlock2D"  # a regular ResNet downsampling block
  - "DownBlock2D"
  - "DownBlock2D"  # a ResNet downsampling block with spatial self-attention
  - "AttnDownBlock2D" # originaly a attention block, changed to a regular block
  up_block_types: !!python/tuple # Describes the type of block to use for upsampling
  - "AttnUpBlock2D"
  - "UpBlock2D"  # a ResNet upsampling block with spatial self-attention
  - "UpBlock2D"
  - "UpBlock2D"  # a regular ResNet upsampling block

# 3. Training
training:
  num_epochs: 100 # Number of epochs to train for
  gradient_accumulation:
    steps: 8 # Number of gradient accumulation steps
  mixed_precision:
    type: 'no' # Type of mixed precision to use
  gradient_clip:
    max_norm: 1.0 # Maximum norm for gradient clipping
  enable_xformers_memory_efficient_attention: False
  optimizer:
    learning_rate: 1.0e-4 # Learning rate for the optimizer
    beta_1: 0.95 # Beta 1 for the AdamW optimizer
    beta_2: 0.999 # Beta 2 for the AdamW optimizer
    weight_decay: 1.0e-6
    eps: 1.0e-8
  lr_scheduler:
    name: "cosine"
    num_warmup_steps: 1000
  noise_scheduler:
    num_train_timesteps: 1000
    beta_schedule: "linear" # originally using "squaredcos_cap_v2", 
    beta_start: 0.0015 # default 0.0001
    beta_end: 0.0195 # default 0.02

# 4. Saving and logging
saving:
  local:
    outputs_dir: 'results/pipelines' # Parent directory for saving outputs
    pipeline_name: 'latentstar_32_ga-8_eps-50_ws-1000' # Name of the pipeline
    checkpoint_frequency: 10000 # How often to save checkpoints (in steps)
    saving_frequency: 10 # How often to save the model (in epochs)
  hf:
    repo_name: 'Latent_Breast' # Name of the HF repo
    model_card_path: 'model_cards/mc_latent.yaml' # # path from experiment directory
logging:
  logger_name: 'wandb' # Name of the logger
  dir_name: 'logs' # Name of the logging directory
  images:
    freq_epochs: 5 # How often to save images (in epochs)
    batch_size: 4 # Batch size for image generation
