# settings:
pretrained_model_name_or_path: runwayml/stable-diffusion-v1-5 #Mandatory, str
pretrained_vae_name_or_path: null # str
# version of the model to use
revision: fp16 # str
# Pretrained tokenizer name or path if not the same as model_name
tokenizer_name: null # str
# data:
instance_data_dir: /home/ricardo/master_thesis/diffusion-models_master/data/images/breast10p_RGB # Mandatory, str
class_data_dir: null # Class images folder for prior preservation
instance_prompt: &a a mammogram # Mandatory, str
class_prompt: null # Class prompt for prior preservation
output_dir: /home/ricardo/master_thesis/diffusion-models_master/results/mammo_abs-64_lr-dream # Mandatory, str
# hub:
push_to_hub: false
hub_token: null
hub_model_id: null # name of the model on the hub (output dir instead)
# logging:
logging_dir: logs # log diredctory
report_to: wandb
validation_prompt: *a # validation prompt for logging
num_validation_images: 4
validation_steps: 500 # how often to log validation images
# prior_preservation:
with_prior_preservation: false
prior_loss_weight: 1.0 # prior preservation loss weight
num_class_images: 100
sample_batch_size: 4
# checkpointing:
checkpointing_steps: 50000 # save state every checkpointing_steps
checkpoints_total_limit: null # limit of total checkpoints to save
resume_from_checkpoint: null # wheather to resume from checkpoint
# training:
seed: 1337 # reproducibility seed
resolution: 512
center_crop: false # center or random crop
train_text_encoder: true # train the text encoder
train_batch_size: 8 # per device
num_train_epochs: 1
max_train_steps: 2500 # can override num_train_epochs
gradient_accumulation_steps: 1
gradient_checkpointing: true
learning_rate: 1.0e-6 # lr after (potential) warmup
scale_lr: False #Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size
lr_scheduler: constant
lr_warmup_steps: 0
lr_num_cycles: 1 #Number of hard resets of the lr in cosine_with_restarts scheduler
lr_power: 1.0 #Power of the polynomial scheduler
# optimizer
use_8bit_adam: true
dataloader_num_workers: 8
adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 1.0e-2
adam_epsilon: 1.0e-8
max_grad_norm: 1.0 # gradient clipping
# memory:
allow_tf32: true # on Ampere GPU only
mixed_precision: 'fp16' # fp16 or bf16
prior_generation_precision: null # fp16, bf16, or fp32
enable_xformers_memory_efficient_attention: true
set_grads_to_none: true
local_rank: -1 # local rank for distributed training
